\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{float}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{colortbl}
\usepackage{tabularray}
\UseTblrLibrary{booktabs}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Automatic Question Generation from Handwritten Lecture Notes Using TrOCR Text Recognition 
and T5 Language Processing}
\author{
\IEEEauthorblockN{\textsuperscript{1}Rommel John H. Ronduen}
\IEEEauthorblockA{
\textit{School of Electrical, Electronics, and} \\
\textit{Computer Engineering} \\
\textit{Map\'ua University}\\
Manila, Philippines \\
rjhronduen@mymail.mapua.edu.ph}
~\\
\and
\IEEEauthorblockN{\textsuperscript{2}Jan Adrian C. Manzanero}
\IEEEauthorblockA{
\textit{School of Electrical, Electronics, and} \\
\textit{Computer Engineering} \\
\textit{Map\'ua University}\\
Manila, Philippines \\
jacmanzanero@mymail.mapua.edu.ph}
~\\
\and
\IEEEauthorblockN{\textsuperscript{3}Analyn N. Yumang}
\IEEEauthorblockA{
\textit{School of Electrical, Electronics, and} \\
\textit{Computer Engineering} \\
\textit{Map\'ua University}\\
Manila, Philippines \\
anyumang@mapua.edu.ph}
}


\maketitle

\begin{abstract}
    This research involves the creation and evaluation of a system that allows 
    for text extraction and automatic question generation (AQG) using a 
    T5 and TrOCR pipeline. With the use of a Raspberry Pi 5, web camera, and a touchscreen display, factoid-
    type questions are created from image captures of single-column handwritten notes that only contain textual information. The 
    T5 large language model (LLM) used was fine-tuned using the 
    Stanford Question Answering Dataset (SQuAD) for facilitating question 
    generation. Evaluation was done using a procured 
    set of handwritten notes on a cybersecurity, and networks undergraduate class described by the word 
    error rate (WER) for content extraction, and the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) as well as the Bilingual Evaluation Understudy (BLEU) for question generation. This research helps to promote the ease of creation of learning materials in learner education.
\end{abstract}

\begin{IEEEkeywords}
Large Language Model, Optical Character Recognition,
Automatic Question Generation, Handwritten Lecture Notes, Raspberry Pi
\end{IEEEkeywords}

\section{Introduction}
\IEEEPARstart{H}{}andwritten lecture notes can be considered as the standard way 
for capturing and facilitating learning. These lecture notes are 
learning artifacts that may contain valuable information that can 
be the basis for other learning elements. One of these learning elements 
involve review questions. These questions are collected in the 
form of quizzes and question banks that allow for the enforcement of 
learning across various fields. Since the introduction of artificial 
intelligence (AI) to education, there have been several advancements 
that are attributed to improving the learning experience of students. 
Automatic question generation (AQG) is one of such advancements. 
It is achieved using large language models (LLMs) from various inputs 
that created questions for the assessment or learning enforcement of 
students.
\\
\indent Despite feasibility in AQG systems, 
these cannot process handwritten notes 
due to the lack of integrated optical character recognition (OCR) 
\cite{Arbaaeen2020}. AQG is derived from large language models (LLMs) 
such as T5, BERT, and GPT-3, which have use cases from 
treebanks \cite{Mesina2020} and classification of certain texts \cite{Padilla2020}.
Existing AQG frameworks are categorized by 
their information-processing methods, yet none address 
handwritten sources \cite{Arbaaeen2020}. While \cite{Ou2022} demonstrated AQG 
on videos using BERT for named entity recognition (NER) on 
transcribed audio, textual or handwritten contexts remain 
unexplored. Other approaches include manual text input 
with part-of-speech (POS) tagging \cite{Moron2021}, rule-based methods 
for programming code \cite{Gaur2023}, and the T5 model 
fine-tuned on SQuAD for text-based AQG \cite{Tsai2021}. OCR, defined as 
image-to-text conversion via convolutional neural networks (CNNs) \cite{Ligsay2022}, has been applied to handwritten Baybayin, medicinal mushroom classification \cite{Sutayco2024}, and expiry date extraction \cite{Manlises2024}. Through \cite{Calimag2023}, a use case combined CNNs with RNNs for real-time inference, evaluated via confusion matrices \cite{Ishikawa2020} \cite{Villaverde2023}. However, Transformer-based OCR (TrOCR) 
\cite{Li2021}, pre-trained on the IAM Handwriting Database, offers superior paragraph-level text recognition over character-
focused CNNs \cite{Mortadi2023}. TrOCRâ€™s robustness positions it as a critical enabler for AQG from handwritten lecture notes, addressing the identified gap in media diversity \cite{Arbaaeen2020}. 
\\
\indent In bridging the gap for processing handwritten lecture notes for AQG, this research has the 
general objective of performing AQG from handwritten lecture
notes using TrOCR text recognition and T5 language processing.
Specifically, this research aims to extract text from
handwritten lecture notes using TrOCR and refine such text 
using the Gemini 1.5 Pro large language model (LLM); utilize 
a fine-tuned base version of T5 on SQuAD for AQG on 
indexed keywords from spaCy and Rapid Automatic Keyword 
Extraction (RAKE) on the refined text; utilize a Raspberry 
Pi 5 within a constructed enclosure with illumination 
for facilitating the system processes and experimental setup;
and lastly, to evaluate the system using the word error 
rate (WER) for the OCR, and the Recall-Oriented Understudy for 
Gisting Evaluation (ROUGE) and the Bilingual Language Understanding 
Evaluation (BLEU) for the quality of AQG.
\\
\indent This system only considers handwritten lecture notes 
that are single-column, diagram and equation free, and written 
in English on plain sheet letter-size paper. It is important to note that 
this system utilizes the base versions of the T5 and TrOCR models where the 
former uses SQuAD for the fine-tuning and Gemini 1.5 Pro for the 
text correction and context completion of OCR-extracted text as well as 
the spaCy-Rake keyword extraction for AQG basis. Also, the system operation is confined 
within the processing capability of the 8-gigabyte model of the Raspberry Pi 5.
Consequently, this system is not expected to behave normally under the usage
multi-column handwritten notes on lined paper on varying sizes. 
A different model for OCR or AQG may lead to differing results as well as 
for the text-enhancing model. Lastly, the usage of alternative hardware 
may lead to different results.
\section{Materials and Methods}
\subsection{Hardware development} 
\subsubsection{Block Diagram}
\hfill 
\begin{figure}[H]
\centerline{\includegraphics[width=0.5\textwidth]{blockdiag.png}}
\vspace{-0.4cm}
\caption{System block diagram.} 
\label{blockdiag}
\end{figure}
\indent Figure \ref{blockdiag} is an illustration for the interconnections 
of the system. The Raspberry Pi 5 facilitates the system algorithm 
through the use of the web camera for the capture of handwritten notes and 
the touchscreen display for the user inputs in dictating the system 
operation. The power supply for the Raspberry Pi 5 allows for the delivery 
of power to the accessory components and main hardware of the system.
    \vspace{0.1cm}
    \subsubsection{Experimental Setup}
    \hfill \\
\vspace{-0.6cm}
\begin{figure}[H]
\centerline{\includegraphics[width=3in]{experimental.png}}
\vspace{-0.4cm}
\caption{Constructed prototype and experimental setup.} 
\label{experimental_setup}
\end{figure}
\indent The constructed prototype is shown in Figure \ref{experimental_setup}.
Experimental setup is done where the system demands handwritten notes 
in the interior of the prototype. An internet connection is required for
the experimental setup. The user is then guided by the front-end 
to capture the handwritten notes. Two halves are captured for the
notes where the first half is the top half and the second half is the
bottom half. The user is then prompted to confirm the captured notes
for the system to proceed with the OCR operation. The system then
proceeds with the OCR operation and the AQG operation. The user is then
prompted to confirm the generated questions for the system to proceed
with the PDF generation. The user is then prompted to download the PDF
file for the generated questions.
\subsection{Software development}
\subsubsection{Algorithm Pipeline}
\hfill
\begin{figure}[H]
\centerline{\includegraphics[width=3in]{pipeline.png}}
\vspace{-0.4cm}
\caption{Algorithm pipeline.} 
\label{pipeline}
\end{figure}
\indent Shown in Figure \ref{pipeline} is the illustration 
of the step-by-step flow of information on the system 
algorithm. An image array in the memory of the system consists 
of the image captures of the handwritten lecture notes in the 
form of OpenCV image objects. These images undergo 
a series of fast means denoising, thresholding, and 
dilation to detect text lines. These text lines are stored in 
another array where TrOCR performs batch inferences to attempt 
extraction from the sources. spaCy and RAKE collects 
relevant keywords and phrases as a basis for the question generation 
which is sent to the T5 model for generation of questions. 
These questions are then stored in an array for portable document 
file (PDF).
\subsubsection{Model Training}
\hfill \\ 
\indent The T5 base model was fine-tuned to the SQuADv1.1 
dataset for the AQG operation with a learning rate of 
$3 \times 10^{-5}$ for eight epochs using the Sequence2Sequence 
trainer. The SQuADv1.1 dataset was loaded using the
datasets library where the chosen input features 
were the context and the question. The output feature 
was the answer for the given pair of inputs.
\subsection{Data Collection}
\vspace{-0.2cm}
\begin{figure}[H]
\centerline{\includegraphics[width=3in]{datacollection.png}}
\vspace{-0.3cm}
\caption{Sample from the test set. Combined image vs. preprocessed image} 
\label{datacollection}
\end{figure}
\indent Shown in Figure \ref{datacollection} is a sample 
from the test set containing 70 handwritten notes collected from undergraduate
classes on computer networks and cybersecurity. These notes were
written in English, single-column, and diagram-free. A total of
1800 individual text lines were extracted from the notes for the
evaluation of the OCR operation through manual transcription
and were saved in a comma-separated value (CSV) format for 
analysis.
\subsection{Testing and Evaluation}
\vspace{-0.2cm}
\begin{table}[H]
\caption{Testing and Evaluation Table for OCR.}
    \centering
    \begin{tblr}{
        colspec={@{}X[1] X[1] X[1] X[1]@{}}, % Makes columns flexible
        column{1} = {c}, % Align first column to center
        hlines,          % Adds horizontal lines
    }
    & \textbf{Student} & \textbf{Model} & \textbf{WER} \\
    0 &  &  &  \\
    ... & ... & ... & ... \\
    70 & ... & ... & ... \\
    &  &   &  Average\\  % Replace X, Y, Z with actual values
    \end{tblr}
    \label{ocrtable}
\end{table}
\indent Shown in Table \ref{ocrtable} is the evaluation of the OCR operation
using the word error rate (WER) for the 70 handwritten notes. 
Testing is done through the collection of the transcribed 
text versus the inference of the model. The WER is
calculated using the formula:
\begin{equation}
    WER = \frac{S + D + I}{N}
\end{equation}
where $S$ is the number of substitutions, $D$ is the number of deletions, $I$ is the number of insertions, and $N$ is the number of words in the reference text.
\begin{table}[H]
    \caption{Testing and Evaluation Table for AQG.}
        \centering
        \begin{tblr}{
            colspec={@{}X[1] X[1] X[1] X[1] X[1] X[1]@{}}, % Makes columns flexible
            column{1} = {c}, % Align first column to center
            hlines,          % Adds horizontal lines
        }
        & \textbf{Keyword} & \textbf{Student} & \textbf{Model} & \textbf{ROUGE} & \textbf{BLEU}\\
        0 &  &  &  & & \\
        ... & ... & ... & ... & ... & ...\\
        70  &     &     &     &     &   \\ 
          &  &  &   & Average & Average\\  % Replace X, Y, Z with actual values
        \end{tblr}
        \label{aqgtable}
        \end{table}
\indent Shown in Table \ref{aqgtable} is the evaluation of the AQG operation
using the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) and the
Bilingual Evaluation Understudy (BLEU) for the 70 handwritten notes.
Testing is done through the collection of the generated questions versus the
reference questions. The ROUGE and BLEU are calculated using the formula:
\begin{equation}
    ROUGE = \frac{1}{N} \sum_{i=1}^{N} \frac{2 \times (P \times R)}{(P + R)}
\end{equation}
\begin{equation}
    BLEU = \frac{1}{N} \sum_{i=1}^{N} \frac{1}{r} \sum_{i=1}^{r} \min \left( \sum_{i=1}^{r} \text{count}_{\text{clip}}(ngram) \right)
\end{equation}
where $P$ is the precision, $R$ is the recall, $N$ is the number of notes, $r$ is the number of reference questions, and $\text{count}_{\text{clip}}$ is the count of the clipped n-grams.
\section{Results and Discussion}
\vspace{-0.2cm}
\begin{figure}[H]
    \centerline{\includegraphics[width=3in]{wer.png}}
    \vspace{-0.5cm}
    \caption{Computed WER and WOER} 
    \label{wer}
    \end{figure}
\indent In Figure \ref{wer}, it was shown that a WER of 0.40 or 40\% 
was realized from the implementation of the TrOCR model with a 
word overlap error rate (WOER) of 0.247 or 24.7\% that 
measures the completeness of the inference versus the reference.
The relatively moderate magnitude of the WER was realized due to the 
natural misalignment of the text lines from the handwritten notes. 
Moreover, the lack of use-case specific fine-tuning to TrOCR 
may have contributed to the WER magnitude. Nevertheless, the
the 60\% accuracy allowed enough room for Gemini 1.5 Pro to correct 
and bridge the gaps in the text digest to form valid questions.
\begin{figure}[H]
    \centerline{\includegraphics[width=3in]{validity.png}}
    \vspace{-0.5cm}
    \caption{Question Validity Rate Pie Chart} 
    \label{validity}
    \end{figure}
\indent In Figure \ref{validity}, it revealed that the generated questions 
by the system are valid 68\% of the time where out of 350 questions, 
238 were valid in terms of coherency, grammar, and it's correlation 
to the subject matter. The remaining invalidity was attributed to 
possibly two aspects. The first is that the T5 model may have too little 
parameters to generate questions that are more coherent and 
grammatically correct. The second is that some keywords may have not been 
corrected by the Gemini 1.5 Pro model which may have led to the
propagation of errors in the generated questions.
\begin{figure}[H]
    \centerline{\includegraphics[width=3in]{eval.png}}
    \vspace{-0.5cm}
    \caption{ROUGE scores and BLEU evaluation} 
    \label{eval}
    \end{figure}
Shown in Figure \ref{eval} is the evaluation of the AQG operation by means
of the ROUGE scores from unigrams to trigrams as well as the BLEU scores. 
A ROUGE-1 score of 0.36 meant that the system decently captures 
the relevant keywords in the same way as students do. 
Moreover, the ROUGE scores for bigrams and trigrams were 0.17
and 0.12 meant that the system was tend to differ from the 
students in terms of the grammatical structure of the questions
because it was realized that the students tended to verbatimly copy 
the structure of the basis context for the questions, while the 
base T5 model generated questions that are more 
dynamic in structure. Lastly, the BLEU score of 0.1 meant that 
the system can pick up context, but not the exact logical structure 
of students.

\section{Conclusion and Recommendations}
\indent It was realized the AQG from handwritten notes 
through TrOCR was possible, where the Raspberry Pi 5 
and the constructed enclosing
was able to cater the processes of the system with 
decent context capture but unpredictable coherency and 
structure. It is recommended that further preprocessing of the image 
captures to improve the text extraction. Also, 
the use of larger versions of TrOCR and T5 is recommended 
alongside the use of a more powerful hardware for the system
through processing power and an improved camera for 
higher image resolutions.

\section*{Acknowledgment}
\indent This research endeavor would not have been possible with 
the incredible support of the researchers' adviser, Engr. Analyn Yumang. Moreover, 
much appreciation is given to the School of Electrical, Electronics, and Computer
Engineering of Map\'ua University for the support and resources provided for the
research.

\bibliographystyle{IEEEtran}
\bibliography{export}
\end{document}
